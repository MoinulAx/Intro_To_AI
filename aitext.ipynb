{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at aitextgen/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "ai= aitextgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A lot of people will say I'm a poor person… but what I'm really going to say is that I'm good at it.'\n",
      "\n",
      "The rapper is not the only one to say he's a poor person. One of his fans went so far as to call him a 'cuckservative'.\n",
      "\n",
      "And his fellow rappers recently called him a 'nigga' and a 'pimp' and even tried to take down him.\n",
      "\n",
      "A lot of people will say I'm a poor person… but what I'm really going to say is that I'm good at it.\n",
      "\n",
      "'I feel like I'm a 'little person' too, a lot of people will say I'm a poor person… but what I'm really going to say is that I'm good at it.\n",
      "\n",
      "'I think the problem is that we're all so stupid. I think we're all so stupid that you can't even get a job.'\n",
      "\n",
      "He added: 'I'm not the first one to say that. I'm not the last one. I've been doing this for a long time. It's not like I'm a 'little person' or anything like that. I'm just trying to be nice to everybody.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mI really like unicorns they are so\u001b[0m much fun!\n",
      "\n",
      "I'm really looking forward to learning more about my own favorite animal.\n",
      "\n",
      "The best part about unicorns is that they are so cute!\n",
      "\n",
      "My favorite thing about unicorns is that they are so cute!\n",
      "\n",
      "I'm always looking for some extra fur or furballs so I can make a bigger cat.\n",
      "\n",
      "My favorite thing about unicorns is that they are so cute!\n",
      "\n",
      "I'm always looking for\n",
      "==========\n",
      "\u001b[1mI really like unicorns they are so\u001b[0m funny!\"\n",
      "\n",
      "I'm not completely sure why I want to buy them, but I hope they're beautiful, and would really make a great gift for anyone who loves them.\n",
      "\n",
      "I love them!\n",
      "\n",
      "[email protected]\n",
      "==========\n",
      "\u001b[1mI really like unicorns they are so\u001b[0m nice to have. I'm having a hard time finding the right one to use everyday. I've found a nice unicorn for me. I use a lot of it and I'm happy that I've found a good one.\n",
      "\n",
      "Rated 5 out of 5 by Anonymous from Good size, fits perfectly. Nice and long, and fits great. I can't find a unicorn to fit my needs. The only thing I can say is, they are a nice\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt=\"I really like unicorns they are so\",max_length=100,n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mYes, because its simply a tool that would elevate stress with the knowledge that you aquire when you learn how to use it\u001b[0m. You'll get as much benefit as you do from learning how to use it.\n",
      "\n",
      "With this knowledge, you'll also gain a better understanding of how the energy of the food you eat is produced.\n",
      "\n",
      "How to Use the Energy of the Food\n",
      "\n",
      "You'll get:\n",
      "\n",
      "Knowledge about how the energy of the food you eat is produced (and thus how it is released)\n",
      "\n",
      "Knowledge about how the energy of the food you eat is released (and thus how it is released) Knowledge about how the energy of the food you eat is released (and thus how it is released)\n",
      "\n",
      "Knowledge that your food is high in calories\n",
      "\n",
      "Knowledge that your food is high in protein\n",
      "\n",
      "Knowledge that your food is high in fats\n",
      "\n",
      "Knowledge that your food is high in sodium\n",
      "\n",
      "Knowledge that your food is high in sugar\n",
      "\n",
      "Knowledge that your food is high in salt\n",
      "\n",
      "Knowledge that your food is high in saturated fat\n",
      "\n",
      "Knowledge that your food is high in monounsaturated fat\n",
      "\n",
      "Knowledge that your food is high in polyunsaturated\n",
      "==========\n",
      "\u001b[1mYes, because its simply a tool that would elevate stress with the knowledge that you aquire when you learn how to use it\u001b[0m. Your brain, which is the most important part of your brain, is not programmed to make this decision.\n",
      "\n",
      "I'm not saying that these techniques are bad. I'm saying that these are a lot more effective than they are.\n",
      "\n",
      "But you can, perhaps, tell me if you're not doing these things well.\n",
      "\n",
      "If you're not doing them well, then you've committed a crime.\n",
      "\n",
      "That's because you're not actually doing them well on your own. You're doing them by listening to the experts.\n",
      "\n",
      "The people in the field that I've been teaching, I have them in my program.\n",
      "\n",
      "But they're not listening to the experts.\n",
      "\n",
      "They're listening to the things that are really, really important.\n",
      "\n",
      "And the people who are, for whatever reason, even more critical to a person's survival than they are to their own survival, are the people who are supposed to be the most critical.\n",
      "\n",
      "What's your response to that?\n",
      "\n",
      "Well, I get a lot of questions this morning.\n",
      "\n",
      "What do you think of my responses to that?\n",
      "\n",
      "==========\n",
      "\u001b[1mYes, because its simply a tool that would elevate stress with the knowledge that you aquire when you learn how to use it\u001b[0m.\n",
      "\n",
      "As for the other, well, I've been using the same things for about a year now. It's been a great experience.\n",
      "\n",
      "I've also been using the same things for a couple of months now.\n",
      "\n",
      "I've now been able to fully experiment with a new technique and I'm starting to feel like I've figured out what I want to do with it.\n",
      "\n",
      "In the meantime, I'm looking forward to learning more about the app, and sharing my experiences in the comments.\n",
      "\n",
      "Thanks!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt=\"Yes, because its simply a tool that would elevate stress with the knowledge that you aquire when you learn how to use it\",n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mwhat is sentience\u001b[0m? How is it sentience to the human mind? How is it sentience to the mind of a living being like me?\n",
      "\n",
      "In addition, there are many other important questions that we need to ask ourselves. We need to learn how to deal with the realities of the world. We need to learn how to cope with the social and economic changes that are happening around us. We need to learn how to understand and work with the problems that bring us to this point.\n",
      "\n",
      "This is a very real and important task, but for those of us in need of help, we can begin by asking ourselves these five questions:\n",
      "\n",
      "When we're in need of help, what is the most important thing we can do to help?\n",
      "\n",
      "How do we deal with this reality?\n",
      "\n",
      "What is the most important thing we can do to help us?\n",
      "\n",
      "When is the most important thing we can do to help you?\n",
      "\n",
      "What is the most important thing we can do to help you to get the help you need?\n",
      "\n",
      "How can we deal with this reality?\n",
      "\n",
      "When is the most important thing we can do to help you?\n",
      "\n",
      "What is the most important thing we can do to help you to get the\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='what is sentience'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mare u an ai\u001b[0m n ai r e u n s i n s i n n s u s i n t e.\n",
      "\n",
      "S e i n t e o f a n d e l e r a t i o n s c e n a t s i n t h e r e f e c t i r e c t i o n s m e n t h e r e a t i o n s t o s c o r f e c t i r e c t i o n o f a n d a s i n t h e r e m e d e s.\n",
      "\n",
      "S o i n t e o f a n d e l e r a t i o n s t h e r e f e c t i r e c t i o n s.\n",
      "\n",
      "S e i n t e o f a n d e l e r a t i o n s t h e r e m e d e s.\n",
      "\n",
      "S o i n t e o f a n d e l e r a t i o n s t h e r e m e d e s.\n",
      "\n",
      "S o i n t e o f a n d e l e r a\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='are u an ai'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer('Shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 123900.39it/s]\n"
     ]
    }
   ],
   "source": [
    "config=GPT2ConfigCPU()\n",
    "\n",
    "ai= aitextgen(tokenizer_file='aitextgen.tokenizer.json',config=config)\n",
    "\n",
    "data=TokenDataset('Shakespeare.txt',tokenizer_file='aitextgen.tokenizer.json',block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                                              \n",
      "\n",
      "\u001b[A\u001b[A                                                                       \n",
      "\u001b[A                                                                          \u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 3.330 — Avg: 3.320:  17%|█▋        | 8300/50000 [12:12<1:01:20, 11.33it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "\n",
      "\u001b[A\u001b[A                                                                       \n",
      "\u001b[A                                                                          \u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 3.330 — Avg: 3.320:  17%|█▋        | 8300/50000 [12:12<1:01:20, 11.33it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "\n",
      "\u001b[A\u001b[A                                                                       \n",
      "\u001b[A                                                                          ==========\n",
      "Loss: 3.330 — Avg: 3.320:  17%|█▋        | 8300/50000 [12:12<1:01:20, 11.33it/s]\n",
      "                                                                                \n",
      "\n",
      "\u001b[A\u001b[A                                                                       \n",
      "\u001b[A                                                                          ,\n",
      "And for this day's life, who, my father,\n",
      "One gracious Henry, believed\n",
      "To seeked, that turn'd in the wits of this day\n",
      "To boothes to thy consul.\n",
      "\n",
      "CATES\n",
      "Loss: 3.330 — Avg: 3.320:  17%|█▋        | 8300/50000 [12:12<1:01:20, 11.33it/s]\n",
      "                                                                                \n",
      "\n",
      "\u001b[A\u001b[A                                                                       \n",
      "\u001b[A                                                                          ==========\n",
      "Loss: 3.330 — Avg: 3.320:  17%|█▋        | 8300/50000 [12:12<1:01:20, 11.33it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.110 — Avg: 3.095: 100%|██████████| 5000/5000 [03:37<00:00, 23.01it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "ai.train(data,batch_size=8,num_steps=5000,generate_every=5000,save_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "If that my brother's power?\n",
      "\n",
      "DUKE OF YORK:\n",
      "Look, thou art to sitt and briefs her death!\n",
      "\n",
      "MERClaim more! My brother, believe,\n",
      "As is dear as I can know,\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "And more last heard to it.\n",
      "\n",
      "ESCALUS:\n",
      "O, the return's the lady.\n",
      "\n",
      "POMPEY:\n",
      "I pray my old Pamid, mistress;\n",
      "Confusion, and not to an a\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "He might says, I know not, I will not see a bucking.\n",
      "\n",
      "ROMEO:\n",
      "At the morning iss and, and the sea-time\n",
      "They persons to believed, and I have heard\n",
      "They\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "My dream, and thy childly kissed my head,\n",
      "Who, else, that did never gone.\n",
      "\n",
      "MONZAGUE:\n",
      "Shall I do it, my lord.\n",
      "\n",
      "KING RICHARD II:\n",
      "He is dead,\n",
      "More\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "The senators of his head, that light, the day\n",
      "As doth lively in your daughter town;\n",
      "This sickly in the king's head\n",
      "He shall garment and married: yet will I have bid him\n",
      "That has,\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "A bark and because\n",
      "Unless Welsed, to be charged to the rest;\n",
      "To sit down the side that the king'st thou fine!\n",
      "If thou rememberables thy chance was a famous\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Madam, my nurse, and my lord;\n",
      "\n",
      "KING EDWARD IV:\n",
      "He shall be, or dost I believe it awake.\n",
      "\n",
      "GLOUCESTER:\n",
      "Messenger:\n",
      "I pray thee, lord, Camillo\n",
      "Hath\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I hope, I will have a bottled hide\n",
      "To feel there it.\n",
      "\n",
      "PAULINA:\n",
      "I am a quickly: and I\n",
      "That's not talking to him, the duke is gone,\n",
      "And I am\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I pray, be gone.\n",
      "\n",
      "ESCALUS:\n",
      "I have done!\n",
      "\n",
      "HERMIONE:\n",
      "Clown:\n",
      "I' the grates of you.\n",
      "\n",
      "CURTIS:\n",
      "\n",
      "ANGELO:\n",
      "M\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Having!\n",
      "\n",
      "AUFIDIUS:\n",
      "That, I say, God's name!\n",
      "\n",
      "CAMILLO:\n",
      "I pray you, sir, sir,\n",
      "You, sir. But on a school, sir,\n",
      "It shall not have your\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10,prompt=\"ROMEO:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
